# dags/two_stock_simple.py

from datetime import datetime, timedelta
import pandas as pd
import yfinance as yf

from airflow import DAG
from airflow.decorators import task
from airflow.models import Variable
from airflow.providers.snowflake.hooks.snowflake import SnowflakeHook
from airflow.operators.trigger_dagrun import TriggerDagRunOperator



# ---------------------------
# Snowflake connection (via Airflow Connection)
# ---------------------------
def return_snowflake_conn():
    hook = SnowflakeHook(snowflake_conn_id='snowflake_conn')
    conn = hook.get_conn()
    return conn.cursor()  


# ---------------------------
# EXTRACT
# ---------------------------
@task
def extract():
    """
    Download raw yfinance data and return it as {symbol: [raw-records]}.
    Ensures columns are single-level and JSON-serializable.
    """
    symbols_csv = Variable.get("stock_symbols", default_var="DIS,NFLX")
    lookback_days = int(Variable.get("lookback_days", default_var="180"))
    symbols = [s.strip() for s in symbols_csv.split(",") if s.strip()]

    out = {}
    for sym in symbols:
        df = yf.download(
            sym,
            period=f"{lookback_days}d",
            interval="1d",
            auto_adjust=False,
            progress=False,
            threads=False,       # avoid cookie/SQLite issue
            group_by="column",   # ensure non-MultiIndex columns
        )

        if df is None or df.empty:
            out[sym] = []
            continue

        df = df.reset_index()

        if isinstance(df.columns, pd.MultiIndex):
            df.columns = [c[0] if isinstance(c, tuple) else c for c in df.columns]

        if "Date" in df.columns:
            df["Date"] = pd.to_datetime(df["Date"]).dt.tz_localize(None).dt.date.astype(str)

        out[sym] = df.to_dict(orient="records")

    return out



@task
def transform(raw):
    """
    Turn raw yfinance records into rows ready for INSERT:
    [SYMBOL, DATE, OPEN, HIGH, LOW, CLOSE, ADJ_CLOSE, VOLUME]
    """
    rows = []
    for sym, recs in raw.items():
        if not recs:
            continue

        df = pd.DataFrame(recs)

        needed = {"Date", "Open", "High", "Low", "Close", "Adj Close", "Volume"}
        if not needed.issubset(df.columns):
            continue
        df = df.dropna(subset=["Close"])
        df = df.rename(columns={"Date": "DATE","Open": "OPEN","High": "HIGH","Low": "LOW",
            "Close": "CLOSE","Adj Close": "ADJ_CLOSE","Volume": "VOLUME", })

        df["DATE"] = pd.to_datetime(df["DATE"]).dt.tz_localize(None).dt.date.astype(str)

        df = df[(df["LOW"] <= df["HIGH"]) & (df["VOLUME"] >= 0)]

        df["SYMBOL"] = sym
        df = df[["SYMBOL", "DATE", "OPEN", "HIGH", "LOW", "CLOSE", "ADJ_CLOSE", "VOLUME"]]

        df = df.where(pd.notna(df), None)

        rows.extend(df.values.tolist())

    return rows


# ---------------------------
# LOAD
# ---------------------------
@task
def load(rows, target_table: str):
    """
    Creates the table if needed, truncates it (full refresh), then inserts all rows.
    Uses a transaction with COMMIT/ROLLBACK.
    """
    # Table DDL â€” safe to run every time thanks to IF NOT EXISTS
    ddl = f"""
    CREATE TABLE IF NOT EXISTS {target_table} (
      SYMBOL       VARCHAR(16)   NOT NULL,
      "DATE"       DATE          NOT NULL,
      OPEN         NUMBER(18,6),
      HIGH         NUMBER(18,6),
      LOW          NUMBER(18,6),
      CLOSE        NUMBER(18,6),
      VOLUME       NUMBER(38,0),
      ADJ_CLOSE    NUMBER(18,6),
      SOURCE       VARCHAR(32)    DEFAULT 'yfinance',
      LOAD_TS      TIMESTAMP_NTZ  DEFAULT CURRENT_TIMESTAMP(),
      PRIMARY KEY (SYMBOL, "DATE")
    );
    """

    insert_sql = f"""
    INSERT INTO {target_table}
      (SYMBOL, "DATE", OPEN, HIGH, LOW, CLOSE, ADJ_CLOSE, VOLUME)
    VALUES
      (%s, %s, %s, %s, %s, %s, %s, %s)
    """

    cur = return_snowflake_conn()
    try:
        cur.execute(ddl)          
        cur.execute("BEGIN")      
        cur.execute(f"TRUNCATE TABLE {target_table}")  

        if rows:
            cur.executemany(insert_sql, rows)       

        cur.execute("COMMIT")    
        return len(rows)
    except Exception:
        cur.execute("ROLLBACK")   
        raise
    finally:
        try:
            cur.close()
        except Exception:
            pass


# ---------------------------
# DAG 
# ---------------------------
with DAG(
    dag_id='TwoStockV2',
    start_date=datetime(2024, 9, 21),
    catchup=False,
    tags=['ETL'],
    schedule='30 2 * * *',           
    default_args={"retries": 1, "retry_delay": timedelta(minutes=5)},
) as dag:

    target_table = "RAW.TWO_STOCK_V2"

    raw_data = extract()
    tidy_rows = transform(raw_data)
    inserted = load(tidy_rows, target_table=target_table)

    trigger_forecast = TriggerDagRunOperator(
        task_id="trigger_forecast",
        trigger_dag_id="TrainPredict",   
        wait_for_completion=False,          
        conf={
            "as_of_date": "{{ ds }}",                       
            "source_table": target_table,                   
            "symbols": "{{ var.value.stock_symbols }}",     # whatever you put in Variables
            "lookback_days": "{{ var.value.lookback_days }}",
        },
    )

    raw_data >> tidy_rows >> inserted >> trigger_forecast
    
